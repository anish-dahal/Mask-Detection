{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as tt\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from Model.ResNet import MaskModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"../Image/multiple8.jpeg\")\n",
    "imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces, _ = mtcnn.detect(imageRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46.91210174560547 248.69467163085938 107.16472625732422\n",
      "  327.5488586425781]\n",
      " [478.0069580078125 227.27606201171875 537.1964721679688\n",
      "  299.21783447265625]\n",
      " [145.26490783691406 171.65464782714844 187.70606994628906\n",
      "  227.37525939941406]\n",
      " [321.5096740722656 119.49620819091797 363.9490051269531\n",
      "  170.91473388671875]\n",
      " [209.72344970703125 98.52619934082031 246.4689483642578\n",
      "  142.97792053222656]\n",
      " [155.6968994140625 7.415914058685303 192.07400512695312\n",
      "  51.974830627441406]\n",
      " [153.16053771972656 52.72328186035156 192.97012329101562\n",
      "  92.74662017822266]\n",
      " [338.32769775390625 24.6259822845459 373.4423522949219 67.51487731933594]\n",
      " [497.06304931640625 69.79196166992188 531.2730712890625\n",
      "  112.69622039794922]\n",
      " [28.838096618652344 122.72537994384766 63.273094177246094\n",
      "  161.37203979492188]\n",
      " [263.78125 119.59635162353516 297.4248352050781 157.0006866455078]\n",
      " [49.6408805847168 42.340721130371094 80.21086120605469 83.36463928222656]\n",
      " [362.2096862792969 48.96070861816406 392.26531982421875\n",
      "  87.70044708251953]\n",
      " [378.49462890625 30.155803680419922 408.6753845214844 67.53424072265625]\n",
      " [553.5696411132812 57.87241744995117 578.1588745117188 91.84117889404297]\n",
      " [444.9849853515625 9.020358085632324 470.5712585449219\n",
      "  39.586849212646484]\n",
      " [534.4248046875 9.972613334655762 556.0556030273438 36.05186462402344]\n",
      " [414.74786376953125 2.438511848449707 435.2772521972656\n",
      "  27.945476531982422]\n",
      " [311.94610595703125 12.224580764770508 330.2506103515625\n",
      "  36.75605010986328]\n",
      " [565.0010986328125 251.7186737060547 581.9842529296875 273.1278076171875]]\n"
     ]
    }
   ],
   "source": [
    "print(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46.91210174560547 248.69467163085938 107.16472625732422 327.5488586425781]\n",
      "[478.0069580078125 227.27606201171875 537.1964721679688 299.21783447265625]\n",
      "[145.26490783691406 171.65464782714844 187.70606994628906\n",
      " 227.37525939941406]\n",
      "[321.5096740722656 119.49620819091797 363.9490051269531 170.91473388671875]\n",
      "[209.72344970703125 98.52619934082031 246.4689483642578 142.97792053222656]\n",
      "[155.6968994140625 7.415914058685303 192.07400512695312 51.974830627441406]\n",
      "[153.16053771972656 52.72328186035156 192.97012329101562 92.74662017822266]\n",
      "[338.32769775390625 24.6259822845459 373.4423522949219 67.51487731933594]\n",
      "[497.06304931640625 69.79196166992188 531.2730712890625 112.69622039794922]\n",
      "[28.838096618652344 122.72537994384766 63.273094177246094\n",
      " 161.37203979492188]\n",
      "[263.78125 119.59635162353516 297.4248352050781 157.0006866455078]\n",
      "[49.6408805847168 42.340721130371094 80.21086120605469 83.36463928222656]\n",
      "[362.2096862792969 48.96070861816406 392.26531982421875 87.70044708251953]\n",
      "[378.49462890625 30.155803680419922 408.6753845214844 67.53424072265625]\n",
      "[553.5696411132812 57.87241744995117 578.1588745117188 91.84117889404297]\n",
      "[444.9849853515625 9.020358085632324 470.5712585449219 39.586849212646484]\n",
      "[534.4248046875 9.972613334655762 556.0556030273438 36.05186462402344]\n",
      "[414.74786376953125 2.438511848449707 435.2772521972656 27.945476531982422]\n",
      "[311.94610595703125 12.224580764770508 330.2506103515625 36.75605010986328]\n",
      "[565.0010986328125 251.7186737060547 581.9842529296875 273.1278076171875]\n"
     ]
    }
   ],
   "source": [
    "for face in faces:\n",
    "    print(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../kaggle/model.pt\", map_location = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tt.Compose([\n",
    "                tt.Resize(size=(256, 256)),\n",
    "                tt.CenterCrop(224),\n",
    "                tt.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "if faces is not None:\n",
    "        for face in faces:\n",
    "            try:\n",
    "                x1 = round(face[0])\n",
    "                y1 = round(face[1])\n",
    "                x2 = round(face[2])\n",
    "                y2 = round(face[3])\n",
    "                face_image = imageRGB[y1 : y2, x1 : x2, :]\n",
    "                img = torch.tensor(face_image)\n",
    "                img = img.permute(2, 0, 1)\n",
    "                # image value range [0, 255] convert it to [0,1]\n",
    "                img = (img-0)/(255-0)\n",
    "                img = transform(img)\n",
    "                img = img[None, :]\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y = model(img.to(device))\n",
    "                    y = F.softmax(y, dim = 1)\n",
    "                    value, pred =torch.max(y, dim = 1)\n",
    "                \n",
    "                \n",
    "                if pred.item() == 0:\n",
    "                    color = (0, 255, 0)\n",
    "                else:\n",
    "                    color = (0, 0, 255)\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), color, 1)\n",
    "                classes = ['with_mask', 'without_mask']\n",
    "                cv2.putText(\n",
    "                            image,\n",
    "                            classes[pred.item()]+f\"({str(round(value.item(), 4))})\",\n",
    "                            (x1, y1-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.5,\n",
    "                            color,\n",
    "                            1,\n",
    "                            cv2.LINE_AA,\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "else:\n",
    "    cv2.putText(\n",
    "            image,\n",
    "            \"No face found\",\n",
    "            (20, 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (255, 255, 0),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "live video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "model = torch.load(\"../kaggle/model.pt\", map_location = device)\n",
    "transform = tt.Compose([\n",
    "                tt.Resize(size=(256, 256)),\n",
    "                tt.CenterCrop(224),\n",
    "                tt.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FPS, 10)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    faces, _ = mtcnn.detect(imageRGB)\n",
    "    \n",
    "    if faces is not None:\n",
    "        for face in faces:\n",
    "            try:\n",
    "                x1 = round(face[0])\n",
    "                y1 = round(face[1])\n",
    "                x2 = round(face[2])\n",
    "                y2 = round(face[3])\n",
    "                face_image = imageRGB[y1 : y2, x1 : x2, :]\n",
    "                img = torch.tensor(face_image)\n",
    "                img = img.permute(2, 0, 1)\n",
    "                # image value range [0, 255] convert it to [0,1]\n",
    "                img = (img-0)/(255-0)\n",
    "                img = transform(img)\n",
    "                img = img[None, :]\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y = model(img.to(device))\n",
    "                    y = F.softmax(y, dim = 1)\n",
    "                    value, pred =torch.max(y, dim = 1)\n",
    "                \n",
    "                \n",
    "                if pred.item() == 0:\n",
    "                    color = (0, 255, 0)\n",
    "                else:\n",
    "                    color = (0, 0, 255)\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), color, 1)\n",
    "                classes = ['with_mask', 'without_mask']\n",
    "                cv2.putText(\n",
    "                            image,\n",
    "                            classes[pred.item()]+f\"({str(round(value.item(), 4))})\",\n",
    "                            (x1, y1-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.5,\n",
    "                            color,\n",
    "                            1,\n",
    "                            cv2.LINE_AA,\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    else:\n",
    "        cv2.putText(\n",
    "                image,\n",
    "                \"No face found\",\n",
    "                (20, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (255, 255, 0),\n",
    "                1,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "    \n",
    "    cv2.imshow('frame', image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
